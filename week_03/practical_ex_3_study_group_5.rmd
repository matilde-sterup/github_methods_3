---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: 'Mina Almasi, Daniel Blumenkranz, Anton Drasbæk Schiønning, Matilde Sterup'
date: "[29-09-2021]"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse, lme4, readbulk, boot)
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

#### 1) Put the data from all subjects into a single data frame  
```{r}
data <- read_bulk(directory = "/Users/minaalmasi/Documents/Cognitive_Science/Methods_3/methods3_code/github_methods_3/week_03/experiment_2")
```

#### 2) Describe the data and construct extra variables from the existing variables  

##### i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.

```{r}
data$correct <- ifelse(data$obj.resp == "e" & data$target.type == "even"|data$obj.resp == "o" & data$target.type == "odd", 1, 0)
```


##### ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  

The following variables in the dataset are:

$~$

__trial.type__:
Whether the trial was a part of the staircase procedure (i.e., before beginning the experiment, coded as "staircase") or whether it was a part of the experiment (coded as "experiment"). ¨


*Class*: This variable is currently coded as a character, but should be coded as a factor if used in modeling. However, we might only be interested in the experimental phase and thus should consider filtering out the *staircase*. 

$~$

__pas__:
Subjective rating of how clearly the target was seen by the participant. Measured using the Perceptual Awareness Scale. According to the article (Andersen et. al) the PAS has 4 categorically different ratings:
  
1. No Experience (NE)
2. Weak Glimpse (WG)
3. Almost Clear Experience (ACE)
4. Clear Experience (CE)


*Class*: This variable is coded as numeric from 1-4 (referring to the 4 levels in the order above). This should be recoded to be a factor, as we are dealing with ordinal data. 

$~$

__trial__:
Number of trial. Resets when the experiment begins. 


*Class*: This variable should also be coded as a factor considering it is nominal data.

$~$

__target.contrast__:
The contrast of the target stimulus relative to the background (adjusted to each participant'
s threshold. After the staircase procedure, this contrast remained fixed for the experimental phase.)

*Class*: We are dealing with continous data between 0 and 1 so it should be numeric class which it is already.

$~$

__cue__: 
36 different combinations of numbers as cues (3 types of cues as indicated by the task setting). 

*Class*: This is nominal data so it should be coded as a factor.

$~$

__task__:
Indicates the task setting. *singles* refers to the task setting being with 2 numbers shown in the cue (e.g., 2:9), *pairs* refers to 4 numbers (e.g., 24:57), and *quadruplet* refers to 8 numbers (e.g., 2468:3579). 

*Class*: This is also nominal data and should be coded as a factor.

$~$

__target_type__: 
Whether the target was an even or odd number. 

*Class*: This is binary data (coded as "even" or "odd") and should be coded as factor.

$~$

__rt.subj__: 
Reaction time for the rating of the _pas_ (i.e., their confidence on how clearly they saw the target). 

*Class*: We are dealing reaction times in seconds which means that it is continuous data that should be coded as numeric.

$~$

__rt.obj__: 
Reaction time of the participant's answer to whether the target was odd or even (i.e., the objective of the experiment).

*Class*: Same as *rt_subj*. That is, reaction times that are continuous and should thus be coded as numeric.

$~$

__obj.resp__: 
The participant's answer to the task

*Class*: This is binary data (coded as "o" = odd and "e" = even) and should be coded as factor.

$~$

__subject__: 
Participant index. 

*Class*: This is nominal data and should therefore be recoded to be a factor.

$~$

__correct__: 
Indicates whether the object response matches the *target_type* (i.e, whether the participant answered correctly.) 

*Class*: This is binary data (coded as 0 = incorrect and 1 = correct) and should be coded as factor.

$~$

__Fixing the Classes__
```{r}
data$trial.type <- as.factor(data$trial.type)
data$pas <- as.factor(data$pas)
data$trial <- as.factor(data$trial)
data$cue <- as.factor(data$cue)
data$task <- as.factor(data$task)
data$target.type <- as.factor(data$target.type)
data$obj.resp <- as.factor(data$obj.resp)
data$subject <- as.factor(data$subject)
data$correct <- as.factor(data$correct)

ls.str(data)
```


##### iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions?

For comparison, a *complete pooling* plot was made:

```{r 1.2iii complete pooling}
# Complete Pooling
staircase <- data %>% filter(trial.type== "staircase")
m <- glm(correct~target.contrast, data = staircase, family = "binomial")
fitted <- fitted(m)
staircase$fitted_values <- fitted

ggplot(staircase, (aes(x = target.contrast, y = as.numeric(as.character(correct)))))+ 
  geom_point()+
  #geom_point(aes(target.contrast, fitted_values))+
  geom_line(aes(target.contrast, fitted_values), color = "blue") +
  facet_wrap(.~subject)+ 
  labs(title = "Complete Pooling", y = "Correct") +
  theme_bw()

```

Secondly, we made a *no pooling* plot for each participant:

```{r 1.2iii no pooling}
# No Pooling
m <- glm(correct~target.contrast + subject + target.contrast:subject, data = staircase, family = "binomial")
fitted <- fitted(m)
staircase$fitted_values <- fitted

ggplot(staircase, (aes(x = target.contrast, y = as.numeric(as.character(correct)))))+ 
  geom_point()+
  geom_line(aes(target.contrast, fitted_values), color = "red") +
  facet_wrap(.~subject)+ 
  labs(title = "No Pooling", y = "Correct") +
  theme_bw()
```


From the *no pooling* plot, we see that the fits are generally very poor - in particular for the low contrast trials. It appears that we do not have enough data to plot the logistic functions. We need more incorrect trials in order for us to do a logistic regression. The consequence of this is that the fitted values do not take shape of the sigmoid function. However, compared to the *complete pooling* plot, *no pooling* is a better fit for each subject.
  
##### iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_  

```{r 1.2iv}
# Partial Pooling
m <- glmer(correct~target.contrast + (target.contrast|subject), data = staircase, family = "binomial")
fitted <- fitted(m)
staircase$fitted_values <- fitted

ggplot(staircase, (aes(x = target.contrast, y = as.numeric(as.character(correct)))))+ 
  geom_point()+
  geom_line(aes(target.contrast, fitted_values), color = "green") +
  facet_wrap(.~subject)+ 
  labs(title = "Partial Pooling", y = "Correct") +
  theme_bw()
```


##### v. in your own words, describe how the partial pooling model allows for a better fit for each subject
  
Compared to the *complete pooling* model, the *partial pooling* model adapts the fit to both the general tendencies in the data as well as the subjective differences. Hence the fit for each subject becomes better than the *complete pooling*, because we account for the fact that they have different baselines and slopes. However, the estimated functions still do not look rather logistic. 

## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)  

#### 1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modelled  
```{r 2.1}
experiment <- data %>% filter(trial.type== "experiment")

subject7 <- experiment %>% filter(subject == 7)
subject10 <- experiment %>% filter(subject == 10)
subject12 <- experiment %>% filter(subject == 12)
subject29 <- experiment %>% filter(subject == 29)

model7 <- lm(rt.obj ~ 1, data = subject7)
qq7 <- ggplot(subject7, aes(sample=residuals(model7))) +
  stat_qq() +
  geom_qq_line() +
  labs(title = "Subject 7") +
  theme_bw()

model10 <- lm(rt.obj ~ 1, data = subject10)
qq10 <- ggplot(subject10, aes(sample=residuals(model10))) +
  stat_qq() +
  geom_qq_line() +
  labs(title = "Subject 10") +
  theme_bw()  

model12 <- lm(rt.obj ~ 1, data = subject12)
qq12 <- ggplot(subject12, aes(sample=residuals(model12))) +
  stat_qq() +
  geom_qq_line() +
  labs(title = "Subject 12") +
  theme_bw()  

model29 <- lm(rt.obj ~ 1, data = subject29)
qq29 <- ggplot(subject29, aes(sample=residuals(model29))) +
  stat_qq() +
  geom_qq_line() +
  labs(title = "Subject 29") +
  theme_bw()  

ggpubr::ggarrange(qq7, qq10, qq12, qq29)
```


##### i. comment on these 
$~$
We should log-transform reaction times, since we have some extreme sample values that distort the image.
$~$

##### ii. does a log-transformation of the response time data improve the Q-Q-plots? 
```{r 2.1ii}
model7log <- lm(log(rt.obj)~1, data = subject7)
qq7log <- ggplot(subject7, aes(sample = residuals(model7log)))+
  stat_qq()+
  geom_qq_line()+
  labs(title = "Subject 7 (log)")+
  theme_bw()

model10log <- lm(log(rt.obj)~1, data = subject10)
qq10log <- ggplot(subject10, aes(sample = residuals(model10log)))+
  stat_qq()+
  geom_qq_line()+
  labs(title = "Subject 10 (log)")+
  theme_bw()

model12log <- lm(log(rt.obj)~1, data = subject12)
qq12log <- ggplot(subject12, aes(sample = residuals(model12log)))+
  stat_qq()+
  geom_qq_line()+
  labs(title = "Subject 12 (log)")+
  theme_bw()

model29log <- lm(log(rt.obj)~1, data = subject29)
qq29log <- ggplot(subject29, aes(sample = residuals(model29log)))+
  stat_qq()+
  geom_qq_line()+
  labs(title = "Subject 29 (log)")+
  theme_bw()

ggpubr::ggarrange(qq7log, qq10log, qq12log, qq29log)

```

The log-transformation clearly improves the qq-plot as the points are much closer to the qq-line. However, the transformation did not fix it completely as points for subject 10 is still very off the line for instance (the distribution is right skewed).

#### 2) Now do a partial pooling model modelling objective response times as dependent on _task_? (set `REML=FALSE` in your `lmer`-specification)

##### i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modelling)  

```{r 2.2}
experiment$task <- relevel(experiment$task, ref = "singles")

task_model <- lmerTest::lmer(rt.obj ~ task + (1|subject) + (1|trial), REML=FALSE, data = experiment)
summary(task_model)
```


We include subject (variance of 0.115) and trial (variance of 0.004) as random effects. Due to likely individual differences in performance, random intercepts are modeled for subject. We have also chosen to include trial with random intercepts due to potential differences in difficulty among trials as well as effects of mental fatigue. However, despite these theoretical arguments, the variance of the random intercepts is low for both subject and trial compared to the residual variance of 8.169. 

$~$
##### ii. explain in your own words what your chosen model says about response times between the different tasks

It seems, that response time only significantly increases when *task* (i.e., the task setting) goes from *singles* to *pairs*. The estimate of the *quadruplet* task is very low, with a large standard error and therefore a large p-value.
    
#### 3) Now add _pas_ and its interaction with _task_ to the fixed effects
```{r 2.3}
pas_interaction_model <- lmerTest::lmer(rt.obj ~ task + pas + task:pas + (1|subject) + (1|trial), REML=FALSE, data = experiment)
summary(pas_interaction_model)
```

##### i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits? 

```{r 2.3i}
amok1_model <- lmerTest::lmer(rt.obj ~ task + pas + task:pas + (1|subject) + (1|trial) + (1|rt.subj), REML=FALSE, data = experiment)

amok2_model <- lmerTest::lmer(rt.obj ~ task + pas + task:pas + (1|subject) + (1|trial) + (1|rt.subj) + (1|even.digit), REML=FALSE, data = experiment)
summary(amok2_model)

amok3_model <- lmerTest::lmer(rt.obj ~ task + pas + task:pas + (1|subject) + (1|trial) + (1|rt.subj) + (1|even.digit) + (1|cue), REML=FALSE, data = experiment)
summary(amok3_model)
```

After adding the fifth random intercept, the model did not converge.


##### ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
```{r 2.3ii}
print(VarCorr(amok3_model), comp='Variance')
print(VarCorr(amok2_model), comp='Variance')
```


The variances of the random intercepts in the singular-model (amok3_model) are generally lower with the variance of the *even.digit* variable being practically equal to zero, which causes the model to not converge (according to the documentation of isSingular).

    
##### iii. in your own words - how could you explain why your model would result in a singular fit? 
  

A too complex model entails singularity. A model is 'too complex' if the number of predictors (fixed and random effects included) is so large that extreme overfitting occurs. 
   
## Exercise 3

#### 1) Initialise a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r 3.1}
data_count <- data %>% 
  group_by(subject, task, pas) %>% 
  summarise("count" = n())

data_count$task <- relevel(data_count$task, ref = "singles")
```

To make the estimates more meaningful, we reordered the levels of task to go from *singles* to *pairs* to *quadruplets*. This is not necessary but merely a question of interpretability. 


#### 2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled  
```{r 3.2}
model1 <- glmer(count~pas*task + (pas|subject), data = data_count, family = poisson, control = glmerControl(optimizer="bobyqa")) 

summary(model1)
```

##### i. which family should be used?  
  
For count data, the "poisson" family should be used. 

##### ii. why is a slope for _pas_ not really being modelled?  
  
The slope of _pas_ is not modelled as a general slope but instead modelled on a level basis due to it being a factor. That is, we have the estimated the slope for *pas2*, *pas3*, *pas2:taskquadruplet* etc.
  
##### iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)
  
The bobyqa optimiser is used as the model failed to converge without it. 

##### iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction  
```{r 3.2iv}
model2 <- glmer(count~pas+task + (pas|subject), data = data_count, family = poisson, control = glmerControl(optimizer="bobyqa")) 

summary(model2)
```

```{r 3.2iv comparison}
#residuals
residuals_model1 <- residuals(model1)
residuals_model2 <- residuals(model2)

#residual variance
rvar_model1<- sum((residuals_model1)^2)
rvar_model2<- sum((residuals_model2)^2)

#residual standard deviation 
sd_model1<- sqrt((sum(residuals_model1)^2/length(residuals_model1)-2))
sd_model2<- sqrt((sum(residuals_model2)^2/length(residuals_model2)-2))

#AICs
AICs <- AIC(model1, model2) #model1 = with interaction, model2 = without interaction

#Table 
tibble("model"=c("with Interaction","Without Interaction"), "residual variance"=c(rvar_model1, rvar_model2), "residual sd"=c(sd_model1, sd_model2), "AIC"=c(AICs[1,2], AICs[2,2]))
```

##### v. indicate which of the two models, you would choose and why
  Both the residual variance, residual standard deviation, and AIC is lower in the model (*model 1*) including the interaction than the model without the interaction (*model 2*). Additionally, the model including the interaction can also be argued for theoretically. It is likely that there is a relationship between the subject's confidence ratings (*pas*) and the type of cue (*task*). For these reasons, we would choose the model with the interaction (*model 1*). 

  
##### vi. based on your chosen model - write a short report on what this says about the distribution of ratings (count) as dependent on _pas_ and _task_  


In the analysis, we investigated the effect of *pas* and *task* on the distribution of ratings (*count*). To test this, we fitted a general linear mixed effects-model with the poisson family as the link function thereby assuming that *count* is distributed as a poisson distribution. Our model has *count* as the outcome variable, *pas* and *task* as fixed effects with those also being included as an interaction term. Additionally, *pas* is modelled as a random slope for each *subject* (i.e., random intercepts are also modelled for each subject). 


Based on the significant estimates in the model output, we can tell that the ratings are not equally distributed across *task* and *pas*. However, *pas* does not solely predict *count*. This makes sense given the experimental design: varying difficulty of task by shortening the duration of the target. On the other hand, *task* solely predicts *count*. This is expected considering that the baseline level is *pas1:tasksingles*. In *task singles*, the cue is simpler and perhaps less distracting which would explain the positive estimates for *taskpairs* (0.231) and *taskquadruplets* (0.346). In other words, the confidence of the participants is lower (that is *pas1*) when the task setting is *pairs* and *quadruplets* than when the task setting is *singles*. Finally, all interaction terms are significant which makes sense considering the participants’ *pas*-scores are affected by the cue type (i.e., *task*). For instance, when the cue type is *quadruplets* it seems that responses are more likely to be near *pas 1* than *pas 4* (the interaction estimates for *quadruplet* are increasingly negative for pas scores going from 1 to 4).   



##### vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing 
```{r 3.2vii}
subject7 <- data_count %>% filter(subject == 7)
subject10 <- data_count %>% filter(subject == 10)
subject12 <- data_count %>% filter(subject == 12)
subject29 <- data_count %>% filter(subject == 29)

estimated_vals <- function(subject_data, model) {
  subject_predict <- predict(model, newdata = subject_data)
  subject_data$estimated_count <- expm1(subject_predict)
  
  plot <- ggplot(subject_data)+
    geom_bar(aes(x=pas, y=estimated_count, fill = task), stat = "identity", position = "dodge")+
    scale_fill_manual(values=c("#4db8ff", "#82E0AA", "#FF6666")) +
    ggtitle(paste("Subject", as.character(subject_data[1,1])))
    theme_bw()
  
  return(plot)
}

subject_7_plot <- estimated_vals(subject7, model1)
subject_10_plot <- estimated_vals(subject10, model1)
subject_12_plot <- estimated_vals(subject12, model1)
subject_29_plot <- estimated_vals(subject29, model1)

ggpubr::ggarrange(subject_7_plot, subject_10_plot, subject_12_plot, subject_29_plot)

```


#### 3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_ 
```{r 3.3}
correct_model1 <- glmer(correct ~ task + (1|subject), family = "binomial", data = experiment)
summary(correct_model1)

```


##### i. does _task_ explain performance?  
Yes. Both *taskpairs* (-0.166) and *taskquadruplet* (-0.241) significantly predict performance (*correct*) (p<.05). The negative coefficients indicate that the amount of correct answers decreases as task complexity increases.   

  
##### ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
```{r 3.3ii}
correct_model2 <- glmer(correct ~ task + pas+ (1|subject), family = "binomial", data = experiment)
summary(correct_model2)
```


By adding *pas* as a main effect along with *task*, *task* no longer significantly predicts *correct*. Instead, *pas* now significantly predicts *correct*. This may be due to the fact that *pas1* is now included in our baseline along with *tasksingles*. This means that the estimates for *taskpairs* and *taskquadruplet* is now only compared within *pas1*.


##### iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
```{r 3.3iii}
correct_model3 <- glmer(correct ~ pas + (1|subject), family = "binomial", data = experiment)
summary(correct_model3)
```

  
##### iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects 
```{r 3.3iv}
correct_model4 <- glmer(correct ~ pas*task + (1|subject), family = "binomial", data = experiment)
summary(correct_model4)
```

  
##### v. describe in your words which model is the best in explaining the variance in accuracy  

The following model comparisons were made: 
```{r 3.3v}
#residuals
residuals_model1 <- residuals(correct_model1)
residuals_model2 <- residuals(correct_model2)
residuals_model3 <- residuals(correct_model3)
residuals_model4 <- residuals(correct_model4)

#residual variance
rvar_model1<- sum((residuals_model1)^2)
rvar_model2<- sum((residuals_model2)^2)
rvar_model3<- sum((residuals_model3)^2)
rvar_model4<- sum((residuals_model4)^2)

#residual standard deviation 
sd_model1<- sqrt((sum(residuals_model1)^2/length(residuals_model1)-2))
sd_model2<- sqrt((sum(residuals_model2)^2/length(residuals_model2)-2))
sd_model3<- sqrt((sum(residuals_model3)^2/length(residuals_model3)-2))
sd_model4<- sqrt((sum(residuals_model4)^2/length(residuals_model4)-2))

#AICs
AICs <- AIC(correct_model1, correct_model2, correct_model3, correct_model4) #model1 = with interaction, model2 = without interaction

#Table 
tibble("model"=c("Correct predicted by Task","Correct predicted by Task & Pas", "Correct predicted by Pas", "Correct predicted by Pas*Task"), "residual variance"=c(rvar_model1, rvar_model2, rvar_model3, rvar_model4), "residual sd"=c(sd_model1, sd_model2, sd_model3, sd_model4), "AIC"=c(AICs[1,2], AICs[2,2], AICs[3,2], AICs[4,2]))
```

From the tibble output, we see that the last three models are very similar when looking at their residual variance, residual standard deviation and AIC. This may suggest that *pas* is a better predictor than *task* alone. However, in choosing the best model, one must consider the theoretical justifications for each model. It could be argued that model 4 should be chosen since there is an interaction term. It would only make sense that *pas* is modulated by the complexity of the *task* since a harder task is likely to result in more uncertainty. 

